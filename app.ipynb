{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent for Market Research & Competitive Analysis\n",
    "\n",
    "This notebook implements an AI agent designed to automate market research. The agent can take a high-level query about a company or technology, search the web for relevant articles, analyze the content, and generate a concise business brief.\n",
    "\n",
    "**Project Goal:** To build a portfolio project demonstrating the practical application of LLMs, Retrieval-Augmented Generation (RAG), and AI agent tool use for a real-world business problem.\n",
    "\n",
    "**Core Components:**\n",
    "1.  **Agent Framework:** LangChain\n",
    "2.  **LLM (for generation):** Google's `gemini-pro` (accessible via free API key)\n",
    "3.  **Embedding Model:** `all-MiniLM-L6-v2` (Open-source from Hugging Face)\n",
    "4.  **Vector Store:** FAISS (In-memory, local, and free)\n",
    "5.  **Tools:**\n",
    "    * Custom Google Search Tool\n",
    "    * Web Scraper Tool\n",
    "    * Yahoo Finance Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we install all the required Python libraries from our `requirements.txt` file hosted on GitHub. This ensures our Colab environment is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Make sure your requirements.txt is in your GitHub repo!\n",
    "# Replace 'your-github-username' with your actual GitHub username.\n",
    "!pip install -q -r https://raw.githubusercontent.com/your-github-username/ai-agent-moat/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Securely Set Up API Keys\n",
    "\n",
    "We need API keys for Google services. We'll use Colab's built-in **Secrets Manager** to handle these securely. This is the best practice and prevents you from ever exposing your keys in the notebook.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Click the **key icon (ðŸ”‘)** in the left sidebar of Colab.\n",
    "2.  Click **\"Add a new secret\"**.\n",
    "3.  Create a secret with the name `GOOGLE_API_KEY` and paste your Google AI Studio API key as the value.\n",
    "4.  Create another secret named `GOOGLE_CSE_ID` with your Custom Search Engine ID.\n",
    "\n",
    "You can get a Google API Key from [Google AI Studio](https://aistudio.google.com/app/apikey) and set up a Custom Search Engine [here](https://programmablesearchengine.google.com/controlpanel/all) to get a Search Engine ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Securely access the API keys\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
    "\n",
    "# Set environment variables for LangChain\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Agent's Tools\n",
    "\n",
    "An agent's power comes from its tools. We will create three tools:\n",
    "1.  **Web Search Tool:** To find relevant articles and sources.\n",
    "2.  **Web Scraper Tool:** To extract the actual content from the URLs found by the search tool.\n",
    "3.  **Yahoo Finance Tool:** To fetch key financial metrics for public companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "from langchain.agents import tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "# Tool 1: Google Search Tool\n",
    "# We will use this object directly in the agent now.\n",
    "search = GoogleSearchAPIWrapper(google_cse_id=GOOGLE_CSE_ID, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Performs a web search using Google and returns the results. NOTE: This tool is now used indirectly by the agent's main logic.\"\"\"\n",
    "    return search.run(query)\n",
    "\n",
    "# Tool 2: Web Scraper Tool\n",
    "@tool\n",
    "def scrape_website(url: str) -> str:\n",
    "    \"\"\"Scrapes the text content of a given website URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Extract text and clean it up\n",
    "            text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
    "            return text[:4000] # Return first 4000 characters to avoid token limits\n",
    "        return f\"Error: Received status code {response.status_code}\"\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error: Could not access the URL. {e}\"\n",
    "\n",
    "# Tool 3: Yahoo Finance Tool\n",
    "@tool\n",
    "def get_stock_info(ticker: str) -> str:\n",
    "    \"\"\"Fetches key financial information for a given stock ticker using Yahoo Finance.\"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        # Extracting a few key metrics\n",
    "        market_cap = info.get('marketCap', 'N/A')\n",
    "        trailing_pe = info.get('trailingPE', 'N/A')\n",
    "        forward_pe = info.get('forwardPE', 'N/A')\n",
    "        long_business_summary = info.get('longBusinessSummary', 'N/A')\n",
    "        return f\"Market Cap: {market_cap}\\nTrailing P/E: {trailing_pe}\\nForward P/E: {forward_pe}\\nBusiness Summary: {long_business_summary}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching stock info for {ticker}: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the RAG Pipeline (In-Memory)\n",
    "\n",
    "This is the core of our project. We will create a `RagAgent` class that encapsulates the logic for Retrieval-Augmented Generation.\n",
    "\n",
    "**How it works:**\n",
    "1.  The agent uses the `web_search` tool to find URLs.\n",
    "2.  It then uses the `scrape_website` tool to get the text from those URLs.\n",
    "3.  All the scraped text is combined into a single corpus.\n",
    "4.  We use our open-source embedding model (`all-MiniLM-L6-v2`) to convert the text into vectors.\n",
    "5.  These vectors are indexed into a temporary **FAISS** vector store.\n",
    "6.  This vector store is then used to create a **retrieval chain**, allowing the agent to \"chat\" with the documents it just found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class RagAgent:\n",
    "    def __init__(self, llm, embeddings_model):\n",
    "        self.llm = llm\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        self.retriever = None\n",
    "        self.retrieval_chain = None\n",
    "        # <<< CHANGE: We will use the search object directly, not the tool decorator\n",
    "        self.search_wrapper = GoogleSearchAPIWrapper(google_cse_id=GOOGLE_CSE_ID, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    def _create_rag_pipeline(self, text_corpus):\n",
    "        \"\"\"Creates a RAG pipeline from a given text corpus.\"\"\"\n",
    "        print(\"\\nStep 1: Splitting documents...\")\n",
    "        docs = self.text_splitter.split_text(text_corpus)\n",
    "        \n",
    "        print(\"Step 2: Creating FAISS vector store...\")\n",
    "        vector_store = FAISS.from_texts(texts=docs, embedding=self.embeddings_model)\n",
    "        self.retriever = vector_store.as_retriever()\n",
    "\n",
    "        print(\"Step 3: Creating retrieval chain...\")\n",
    "        system_prompt = (\n",
    "            \"You are an expert market research analyst. Use the following retrieved context \"\n",
    "            \"to answer the user's question. If you don't know the answer, say you don't know. \"\n",
    "            \"Provide a detailed, well-structured answer based only on the context provided.\"\n",
    "            \"\\n\\n{context}\"\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "        \n",
    "        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)\n",
    "        self.retrieval_chain = create_retrieval_chain(self.retriever, question_answer_chain)\n",
    "        print(\"RAG pipeline is ready.\")\n",
    "\n",
    "    def run(self, query):\n",
    "        print(f\"Executing query: {query}\")\n",
    "        \n",
    "        # <<< CHANGE: Create a simpler, keyword-focused search query.\n",
    "        # For this example, we'll extract the core entity from the user's query.\n",
    "        # A more advanced agent could use an LLM call to generate a better query.\n",
    "        search_query = f\"market analysis and recent news for {query.split('for ')[-1].split('.')[0]}\"\n",
    "        print(f\"Generated search query: {search_query}\")\n",
    "\n",
    "        # <<< CHANGE: Use .results() to get structured data, not .run()\n",
    "        search_results = self.search_wrapper.results(search_query, num_results=5)\n",
    "        \n",
    "        if not search_results:\n",
    "            return \"The web search returned no results. Please try a different query.\"\n",
    "        \n",
    "        # <<< CHANGE: Extract links from the structured results\n",
    "        urls = [result['link'] for result in search_results if 'link' in result]\n",
    "        print(f\"Found {len(urls)} URLs.\")\n",
    "\n",
    "        # Scrape content from the top 3 URLs\n",
    "        scraped_content = []\n",
    "        for url in urls[:3]:\n",
    "            print(f\"Scraping {url}...\")\n",
    "            content = scrape_website.run(url)\n",
    "            if content and not content.startswith(\"Error\"):\n",
    "                scraped_content.append(content)\n",
    "        \n",
    "        if not scraped_content:\n",
    "            return \"Could not retrieve any content from the web. Please try another query.\"\n",
    "        \n",
    "        full_text = \"\\n\\n---\\n\\n\".join(scraped_content)\n",
    "        self._create_rag_pipeline(full_text)\n",
    "        \n",
    "        print(\"\\nSynthesizing final answer...\")\n",
    "        response = self.retrieval_chain.invoke({\"input\": query})\n",
    "        \n",
    "        return response['answer']\n",
    "\n",
    "# Initialize models\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the agent\n",
    "market_research_agent = RagAgent(llm=llm, embeddings_model=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run the Agent and Generate a Business Brief\n",
    "\n",
    "Now it's time to test our agent. Let's give it a complex query and see what kind of analysis it can generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Generate a market analysis for NVIDIA (NVDA). Identify key growth drivers and summarize recent news.\"\n",
    "\n",
    "final_brief = market_research_agent.run(user_query)\n",
    "\n",
    "print(\"\\n--- FINAL BUSINESS BRIEF ---\\n\")\n",
    "print(final_brief)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Using the Yahoo Finance Tool\n",
    "\n",
    "We can also use our tools individually. Let's quickly get some financial data for another company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amd_info = get_stock_info.run(\"AMD\")\n",
    "print(\"--- AMD Financial Info ---\")\n",
    "print(amd_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
