{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent for Market Research & Competitive Analysis\n",
    "\n",
    "This notebook implements an AI agent designed to automate market research. The agent can take a high-level query about a company or technology, search the web for relevant articles, analyze the content, and generate a concise business brief.\n",
    "\n",
    "**Project Goal:** To build a portfolio project demonstrating the practical application of LLMs, Retrieval-Augmented Generation (RAG), and AI agent tool use for a real-world business problem.\n",
    "\n",
    "**Core Components:**\n",
    "1.  **Agent Framework:** LangChain\n",
    "2.  **LLM (for generation):** Google's `gemini-pro` (accessible via free API key)\n",
    "3.  **Embedding Model:** `all-MiniLM-L6-v2` (Open-source from Hugging Face)\n",
    "4.  **Vector Store:** FAISS (In-memory, local, and free)\n",
    "5.  **Tools:**\n",
    "    * Custom Google Search Tool\n",
    "    * Web Scraper Tool\n",
    "    * Yahoo Finance Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we install all the required Python libraries. This cell includes a check to see if the libraries are already installed, saving time on subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Check if a key library is installed. If not, run the installation.\n",
    "if not importlib.util.find_spec(\"langchain\"):\n",
    "  print(\"Installing dependencies...\")\n",
    "  # Use your GitHub username here\n",
    "  !pip install -q -r https://raw.githubusercontent.com/eriktaylor/ai-agent-moat/main/requirements.txt\n",
    "else:\n",
    "  print(\"Dependencies are already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Securely Set Up API Keys\n",
    "\n",
    "We need API keys for Google services. We'll use Colab's built-in **Secrets Manager** to handle these securely. This is the best practice and prevents you from ever exposing your keys in the notebook.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Click the **key icon (ðŸ”‘)** in the left sidebar of Colab.\n",
    "2.  Click **\"Add a new secret\"**.\n",
    "3.  Create a secret with the name `GOOGLE_API_KEY` and paste your Google AI Studio API key as the value.\n",
    "4.  Create another secret named `GOOGLE_CSE_ID` with your Custom Search Engine ID.\n",
    "\n",
    "You can get a Google API Key from [Google AI Studio](https://aistudio.google.com/app/apikey) and set up a Custom Search Engine [here](https://programmablesearchengine.google.com/controlpanel/all) to get a Search Engine ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Securely access the API keys\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
    "\n",
    "# Set environment variables for LangChain\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Agent's Tools\n",
    "\n",
    "An agent's power comes from its tools. We will create three tools:\n",
    "1.  **Web Search Tool:** To find relevant articles and sources.\n",
    "2.  **Web Scraper Tool:** To extract the actual content from the URLs found by the search tool.\n",
    "3.  **Yahoo Finance Tool:** To fetch key financial metrics for public companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "from langchain.agents import tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "# Tool 1: Google Search Tool\n",
    "# This object will be used directly by our agent's logic.\n",
    "search = GoogleSearchAPIWrapper(google_cse_id=GOOGLE_CSE_ID, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Tool 2: Web Scraper Tool\n",
    "@tool\n",
    "def scrape_website(url: str) -> str:\n",
    "    \"\"\"Scrapes the text content of a given website URL.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
    "            if len(text) < 200: # Simple check for meaningful content\n",
    "                return f\"Error: Content from {url} is too short or likely requires JavaScript.\"\n",
    "            return text[:4000]\n",
    "        return f\"Error: Received status code {response.status_code}\"\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error: Could not access the URL. {e}\"\n",
    "\n",
    "# Tool 3: Yahoo Finance Tool\n",
    "@tool\n",
    "def get_stock_info(ticker: str) -> str:\n",
    "    \"\"\"Fetches key financial information for a given stock ticker using Yahoo Finance.\"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        market_cap = info.get('marketCap', 'N/A')\n",
    "        trailing_pe = info.get('trailingPE', 'N/A')\n",
    "        forward_pe = info.get('forwardPE', 'N/A')\n",
    "        long_business_summary = info.get('longBusinessSummary', 'N/A')\n",
    "        return f\"FINANCIAL DATA:\\nMarket Cap: {market_cap}\\nTrailing P/E: {trailing_pe}\\nForward P/E: {forward_pe}\\nBusiness Summary: {long_business_summary}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching stock info for {ticker}: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the RAG Pipeline (In-Memory)\n",
    "\n",
    "This is the core of our project. We will create a `RagAgent` class that encapsulates the logic for Retrieval-Augmented Generation.\n",
    "It now uses a **Hybrid Search Strategy** to gather both recent headlines and detailed documents, and it explicitly calls the financial data tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import re\n",
    "\n",
    "class RagAgent:\n",
    "    def __init__(self, llm, embeddings_model):\n",
    "        self.llm = llm\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        self.retriever = None\n",
    "        self.retrieval_chain = None\n",
    "        self.search_wrapper = GoogleSearchAPIWrapper(google_cse_id=GOOGLE_CSE_ID, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "    def _create_rag_pipeline(self, text_corpus):\n",
    "        \"\"\"Creates a RAG pipeline from a given text corpus.\"\"\"\n",
    "        print(\"\\nStep 1: Splitting combined text...\")\n",
    "        docs = self.text_splitter.split_text(text_corpus)\n",
    "        \n",
    "        print(\"Step 2: Creating FAISS vector store...\")\n",
    "        vector_store = FAISS.from_texts(texts=docs, embedding=self.embeddings_model)\n",
    "        self.retriever = vector_store.as_retriever()\n",
    "\n",
    "        print(\"Step 3: Creating retrieval chain with sophisticated prompt...\")\n",
    "        # <<< CHANGE: Implement the advanced, structured prompt >>>\n",
    "        system_prompt = (\n",
    "            \"You are an expert financial analyst writing a concise investment brief. \"\n",
    "            \"Use the provided context, which includes recent news headlines, full articles, and key financial metrics, to generate your report. \"\n",
    "            \"The report MUST be structured with the following sections:\\n\"\n",
    "            \"1. **Recent Developments:** A summary of the most important recent news from the context.\\n\"\n",
    "            \"2. **SWOT Analysis:** A bulleted list of the company's Strengths, Weaknesses, Opportunities, and Threats based on the context.\\n\"\n",
    "            \"3. **Valuation & Moat:** An analysis of the company's current valuation, referencing specific metrics like the P/E ratio if available in the context. Briefly comment on the company's competitive moat (its long-term competitive advantages).\"\n",
    "            \"\\n\\n{context}\"\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "        \n",
    "        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)\n",
    "        self.retrieval_chain = create_retrieval_chain(self.retriever, question_answer_chain)\n",
    "        print(\"RAG pipeline is ready.\")\n",
    "\n",
    "    def run(self, query):\n",
    "        print(f\"Executing query: {query}\")\n",
    "        # Extract the ticker symbol from the query using regex\n",
    "        ticker_match = re.search(r'\\((.*?)\\)', query)\n",
    "        ticker = ticker_match.group(1) if ticker_match else \"\"\n",
    "        entity = query.split('for ')[-1].split('(')[0].strip()\n",
    "        \n",
    "        all_text_content = []\n",
    "\n",
    "        # <<< CHANGE: Explicitly call the financial data tool >>>\n",
    "        if ticker:\n",
    "            print(f\"\\n--- Getting Financial Data for {ticker} ---\")\n",
    "            financial_data = get_stock_info.run(ticker)\n",
    "            if not financial_data.startswith(\"Error\"):\n",
    "                all_text_content.append(financial_data)\n",
    "                print(\"Successfully collected financial data.\")\n",
    "\n",
    "        # Tier 1: \"Headline Scan\" for recent news from major outlets\n",
    "        print(\"\\n--- Tier 1: Headline Scan ---\")\n",
    "        headline_query = f'\"{entity}\" recent news'\n",
    "        print(f\"Searching for headlines with: {headline_query}\")\n",
    "        headline_results = self.search_wrapper.results(headline_query, num_results=5)\n",
    "        if headline_results:\n",
    "            for result in headline_results:\n",
    "                all_text_content.append(f\"Headline: {result.get('title', '')}\\nSnippet: {result.get('snippet', '')}\")\n",
    "            print(f\"Collected {len(headline_results)} headlines and snippets.\")\n",
    "\n",
    "        # Tier 2: \"Deep Dive\" for detailed, scrape-friendly content\n",
    "        print(\"\\n--- Tier 2: Deep Dive ---\")\n",
    "        deep_dive_query = f'\\\"{entity}\\\" market analysis OR in-depth report filetype:pdf OR site:globenewswire.com OR site:prnewswire.com'\n",
    "        print(f\"Searching for documents with: {deep_dive_query}\")\n",
    "        deep_dive_results = self.search_wrapper.results(deep_dive_query, num_results=5)\n",
    "        if deep_dive_results:\n",
    "            urls = [result['link'] for result in deep_dive_results if 'link' in result]\n",
    "            print(f\"Found {len(urls)} documents for deep dive.\")\n",
    "            for url in urls[:3]: # Scrape top 3\n",
    "                print(f\"Scraping {url}...\")\n",
    "                content = scrape_website.run(url)\n",
    "                if content and not content.startswith(\"Error\"):\n",
    "                    all_text_content.append(content)\n",
    "                else:\n",
    "                    print(content) # Print error if scraping fails\n",
    "\n",
    "        if not all_text_content:\n",
    "            return \"Could not retrieve any content from the web. Please try another query.\"\n",
    "        \n",
    "        full_text = \"\\n\\n---\\n\\n\".join(all_text_content)\n",
    "        self._create_rag_pipeline(full_text)\n",
    "        \n",
    "        print(\"\\nSynthesizing final answer from combined context...\")\n",
    "        response = self.retrieval_chain.invoke({\"input\": query})\n",
    "        \n",
    "        return response['answer']\n",
    "\n",
    "# Initialize models\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.2) # Lower temperature for more factual output\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the agent\n",
    "market_research_agent = RagAgent(llm=llm, embeddings_model=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run the Agent and Generate a Business Brief\n",
    "\n",
    "Now it's time to test our agent. Let's give it a complex query and see what kind of analysis it can generate. The output will be displayed in a formatted, scrollable box for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "user_query = \"Generate a market analysis for NVIDIA (NVDA). Identify key growth drivers and summarize recent news.\"\n",
    "\n",
    "final_brief = market_research_agent.run(user_query)\n",
    "\n",
    "print(\"\\n--- FINAL BUSINESS BRIEF ---\\n\")\n",
    "\n",
    "# <<< CHANGE: Display the output in a formatted, scrollable box >>>\n",
    "formatted_brief = f\"\"\"\n",
    "<div style=\"border: 1px solid #e0e0e0; border-radius: 8px; padding: 20px; max-height: 500px; overflow-y: auto; white-space: pre-wrap; font-family: 'SF Pro Text', 'Inter', sans-serif; line-height: 1.6; background-color: #f9fafb;\">\n",
    "{final_brief}\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(formatted_brief))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
