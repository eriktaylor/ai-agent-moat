{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent for Market Research & Competitive Analysis\n",
    "\n",
    "This notebook implements an AI agent designed to automate market research. The agent can take a high-level query about a company or technology, search the web for relevant articles, analyze the content, and generate a concise business brief.\n",
    "\n",
    "**Core Components:**\n",
    "1.  **Agent Framework:** LangChain\n",
    "2.  **LLM (for generation):** Google's `gemini-2.5-flash` (for better rate limits)\n",
    "3.  **Embedding Model:** `all-MiniLM-L6-v2` (Open-source from Hugging Face)\n",
    "4.  **Vector Store:** FAISS (In-memory, local, and free)\n",
    "5.  **Tools:**\n",
    "    * Custom Google Search Tool\n",
    "    * Web Scraper Tool (Now with PDF support)\n",
    "    * Yahoo Finance Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we install all the required Python libraries. This cell includes a check to see if the libraries are already installed, saving time on subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Check if a key library is installed. If not, run the installation.\n",
    "if not importlib.util.find_spec(\"langchain\"):\n",
    "  print(\"Installing dependencies...\")\n",
    "  # Use your GitHub username here\n",
    "  !pip install -q -r https://raw.githubusercontent.com/eriktaylor/ai-agent-moat/main/requirements.txt\n",
    "else:\n",
    "  print(\"Dependencies are already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Securely Set Up API Keys\n",
    "\n",
    "We need API keys for Google services. We'll use Colab's built-in **Secrets Manager** to handle these securely. This is the best practice and prevents you from ever exposing your keys in the notebook.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Click the **key icon (ðŸ”‘)** in the left sidebar of Colab.\n",
    "2.  Click **\"Add a new secret\"**.\n",
    "3.  Create a secret with the name `GOOGLE_API_KEY` and paste your Google AI Studio API key as the value.\n",
    "4.  Create another secret named `GOOGLE_CSE_ID` with your Custom Search Engine ID.\n",
    "\n",
    "You can get a Google API Key from [Google AI Studio](https://aistudio.google.com/app/apikey) and set up a Custom Search Engine [here](https://programmablesearchengine.google.com/controlpanel/all) to get a Search Engine ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Securely access the API keys\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
    "\n",
    "# Set environment variables for LangChain\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Agent's Tools\n",
    "\n",
    "An agent's power comes from its tools. We will create three tools:\n",
    "1.  **Web Search Tool:** To find relevant articles and sources.\n",
    "2.  **Web Scraper Tool:** To extract the actual content from the URLs found by the search tool.\n",
    "3.  **Yahoo Finance Tool:** To fetch key financial metrics for public companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "from langchain.agents import tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "\n",
    "# Tool 1: Google Search Tool\n",
    "search = GoogleSearchAPIWrapper(google_cse_id=GOOGLE_CSE_ID, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Upgraded scraper tool with PDF handling\n",
    "@tool\n",
    "def scrape_website(url: str) -> str:\n",
    "    \"\"\"Scrapes text from HTML websites and extracts text from PDF files.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "\n",
    "        if 'application/pdf' in content_type or url.lower().endswith('.pdf'):\n",
    "            # It's a PDF, use PyMuPDF to extract text\n",
    "            with fitz.open(stream=io.BytesIO(response.content), filetype='pdf') as doc:\n",
    "                text = \"\".join(page.get_text() for page in doc)\n",
    "            return text[:8000] # Return a larger chunk for detailed PDFs\n",
    "        elif 'text/html' in content_type:\n",
    "            # It's HTML, use BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
    "            if len(text) < 200:\n",
    "                return f\"Error: HTML content from {url} is too short.\"\n",
    "            return text[:4000]\n",
    "        else:\n",
    "            return f\"Error: Unsupported content type '{content_type}' at {url}\"\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error: Could not access the URL. {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An unexpected error occurred while processing {url}. {e}\"\n",
    "\n",
    "# Tool 3: Yahoo Finance Tool\n",
    "@tool\n",
    "def get_stock_info(ticker: str) -> str:\n",
    "    \"\"\"Fetches key financial information for a given stock ticker using Yahoo Finance.\"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        market_cap = info.get('marketCap', 'N/A')\n",
    "        trailing_pe = info.get('trailingPE', 'N/A')\n",
    "        forward_pe = info.get('forwardPE', 'N/A')\n",
    "        long_business_summary = info.get('longBusinessSummary', 'N/A')\n",
    "        return f\"### KEY FINANCIAL DATA ###\\nMarket Cap: {market_cap}\\nTrailing P/E: {trailing_pe}\\nForward P/E: {forward_pe}\\nBusiness Summary: {long_business_summary}\\n### END FINANCIAL DATA ###\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching stock info for {ticker}: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Research Agent\n",
    "\n",
    "The `ResearchAgent` class now encapsulates the entire research process, with distinct methods for different types of analysis. It also includes a caching mechanism to improve speed and avoid hitting API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import re\n",
    "\n",
    "class ResearchAgent:\n",
    "    def __init__(self, llm, embeddings_model):\n",
    "        self.llm = llm\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        self.search_wrapper = GoogleSearchAPIWrapper(google_cse_id=GOOGLE_CSE_ID, google_api_key=GOOGLE_API_KEY)\n",
    "        self.cache = {}\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clears the agent's cache.\"\"\"\n",
    "        print(\"Cache cleared.\")\n",
    "        self.cache = {}\n",
    "\n",
    "    def _get_context(self, query):\n",
    "        \"\"\"Helper function to gather all context data.\"\"\"\n",
    "        context_cache_key = f\"context_{query}\"\n",
    "        if context_cache_key in self.cache:\n",
    "            print(\"Returning cached context.\")\n",
    "            return self.cache[context_cache_key]\n",
    "\n",
    "        ticker_match = re.search(r'\\((.*?)\\)', query)\n",
    "        ticker = ticker_match.group(1) if ticker_match else \"\"\n",
    "        entity = query.split('for ')[-1].split('(')[0].strip()\n",
    "        \n",
    "        all_text_content = []\n",
    "        if ticker:\n",
    "            print(f\"--- Getting Financial Data for {ticker} ---\")\n",
    "            financial_data = get_stock_info.run(ticker)\n",
    "            if not financial_data.startswith(\"Error\"):\n",
    "                all_text_content.insert(0, financial_data)\n",
    "                print(\"Successfully collected and prioritized financial data.\")\n",
    "\n",
    "        print(\"--- Tier 1: Headline Scan ---\")\n",
    "        headline_query = f'\"{entity}\" recent news'\n",
    "        headline_results = self.search_wrapper.results(headline_query, num_results=5)\n",
    "        if headline_results:\n",
    "            all_text_content.extend([f\"Headline: {r.get('title', '')}\\nSnippet: {r.get('snippet', '')}\" for r in headline_results])\n",
    "            print(f\"Collected {len(headline_results)} headlines and snippets.\")\n",
    "\n",
    "        print(\"--- Tier 2: Deep Dive ---\")\n",
    "        deep_dive_query = f'\\\"{entity}\\\" market analysis OR in-depth report filetype:pdf OR site:globenewswire.com OR site:prnewswire.com'\n",
    "        deep_dive_results = self.search_wrapper.results(deep_dive_query, num_results=3)\n",
    "        if deep_dive_results:\n",
    "            urls = [result['link'] for result in deep_dive_results if 'link' in result]\n",
    "            for url in urls:\n",
    "                print(f\"Scraping {url}...\")\n",
    "                content = scrape_website.run(url)\n",
    "                if content and not content.startswith(\"Error\"):\n",
    "                    all_text_content.append(content)\n",
    "                    print(f\"Successfully scraped content from {url}\")\n",
    "                else:\n",
    "                    print(content)\n",
    "        \n",
    "        full_text = \"\\n\\n---\\n\\n\".join(all_text_content)\n",
    "        self.cache[context_cache_key] = full_text\n",
    "        return full_text\n",
    "\n",
    "    def _create_rag_chain(self, system_prompt, text_corpus):\n",
    "        \"\"\"Helper to create a RAG chain with a specific prompt.\"\"\"\n",
    "        docs = self.text_splitter.split_text(text_corpus)\n",
    "        vector_store = FAISS.from_texts(texts=docs, embedding=self.embeddings_model)\n",
    "        retriever = vector_store.as_retriever()\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "        Youtube_chain = create_stuff_documents_chain(self.llm, prompt)\n",
    "        return create_retrieval_chain(retriever, Youtube_chain)\n",
    "\n",
    "    def generate_outlook(self, query):\n",
    "        print(\"\\nGenerating Investment Outlook...\")\n",
    "        text_corpus = self._get_context(query)\n",
    "        if not text_corpus:\n",
    "            return \"Could not gather context for outlook generation.\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a neutral financial analyst. Based ONLY on the provided text (which includes financial data and news), summarize the investment outlook. \"\n",
    "            \"Structure your response into two sections: 'Bullish Case (Potential Positives)' and 'Bearish Case (Potential Risks)'. \"\n",
    "            \"Reference specific data points like P/E ratios or news headlines from the context to support your points. \"\n",
    "            \"DO NOT give financial advice or make a final 'buy' or 'sell' recommendation. This is an objective summary of the data provided.\"\n",
    "            \"\\n\\n{context}\"\n",
    "        )\n",
    "        rag_chain = self._create_rag_chain(system_prompt, text_corpus)\n",
    "        response = rag_chain.invoke({\"input\": query})\n",
    "        return response['answer']\n",
    "\n",
    "    def generate_brief(self, query):\n",
    "        print(\"\\nGenerating Comprehensive Business Brief...\")\n",
    "        text_corpus = self._get_context(query)\n",
    "        if not text_corpus:\n",
    "            return \"Could not gather context for brief generation.\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are an expert market research analyst. Use the provided context to generate a detailed business brief. \"\n",
    "            \"The report MUST be structured with the following sections:\\n\"\n",
    "            \"1. **Recent Developments:** A summary of the most important recent news from the context.\\n\"\n",
    "            \"2. **SWOT Analysis:** A bulleted list of the company's Strengths, Weaknesses, Opportunities, and Threats based on the context.\\n\"\n",
    "            \"3. **Competitive Moat:** Briefly comment on the company's long-term competitive advantages based on the summary and SWOT analysis.\"\n",
    "            \"\\n\\n{context}\"\n",
    "        )\n",
    "        rag_chain = self._create_rag_chain(system_prompt, text_corpus)\n",
    "        response = rag_chain.invoke({\"input\": query})\n",
    "        return response['answer']\n",
    "\n",
    "# Initialize models and agent\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "research_agent = ResearchAgent(llm=llm, embeddings_model=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run the Three-Step Analysis\n",
    "\n",
    "Now we execute our new, structured workflow. First, we'll display the raw financial data. Then, we'll generate the investment outlook, and finally, the comprehensive business brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "user_query = \"Generate a market analysis for NVIDIA (NVDA). Identify key growth drivers and summarize recent news.\"\n",
    "\n",
    "# To get fresh data and not use the cache, you can uncomment the next line:\n",
    "# research_agent.clear_cache()\n",
    "\n",
    "# --- Step 1: Display Raw Financial Data ---\n",
    "print(\"--- 1. KEY FINANCIAL DATA ---\")\n",
    "ticker_match = re.search(r'\\((.*?)\\)', user_query)\n",
    "ticker = ticker_match.group(1) if ticker_match else \"\"\n",
    "financial_data_raw = get_stock_info.run(ticker) if ticker else \"No ticker found.\"\n",
    "display(HTML(f\"<pre style='white-space: pre-wrap; font-family: monospace; background-color: #f0f0f0; padding: 10px; border-radius: 5px;'>{financial_data_raw}</pre>\"))\n",
    "\n",
    "# --- Step 2: Generate Investment Outlook ---\n",
    "print(\"\\n--- 2. AI-GENERATED INVESTMENT OUTLOOK ---\")\n",
    "outlook = research_agent.generate_outlook(user_query)\n",
    "display(HTML(f\"<div style='border: 1px solid #444; border-radius: 8px; padding: 20px; max-height: 500px; overflow-y: auto; white-space: pre-wrap; font-family: \\\"SF Pro Text\\\", \\\"Inter\\\", sans-serif; line-height: 1.6; background-color: #2c2c2e; color: #f0f0f0;'>{outlook}</div>\"))\n",
    "\n",
    "# --- Step 3: Generate Business Brief & SWOT Analysis ---\n",
    "print(\"\\n--- 3. COMPREHENSIVE BUSINESS BRIEF ---\")\n",
    "brief = research_agent.generate_brief(user_query)\n",
    "display(HTML(f\"<div style='border: 1px solid #444; border-radius: 8px; padding: 20px; max-height: 500px; overflow-y: auto; white-space: pre-wrap; font-family: \\\"SF Pro Text\\\", \\\"Inter\\\", sans-serif; line-height: 1.6; background-color: #2c2c2e; color: #f0f0f0;'>{brief}</div>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
