{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent for Market Research & Competitive Analysis\n",
    "\n",
    "This notebook implements an AI agent designed to automate market research. The agent takes a company name and stock ticker, performs a multi-faceted analysis by gathering real-time financial data and news, and presents the results from two distinct investor perspectives: a short-term Market Investor and a long-term Value Investor.\n",
    "\n",
    "**Core Components:**\n",
    "1.  **Agent Framework:** LangChain\n",
    "2.  **LLM (for generation):** Google's `gemini-2.5-flash` (for better rate limits)\n",
    "3.  **Embedding Model:** `all-MiniLM-L6-v2` (Open-source from Hugging Face)\n",
    "4.  **Vector Store:** FAISS (In-memory, local, and free)\n",
    "5.  **Tools:**\n",
    "    * Custom Google Search Tool\n",
    "    * Web Scraper Tool (with PDF support)\n",
    "    * Yahoo Finance Tool (with EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we install all the required Python libraries. This cell includes a check to see if the libraries are already installed, saving time on subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Check if a key library is installed. If not, run the installation.\n",
    "if not importlib.util.find_spec(\"langchain\"):\n",
    "  print(\"Installing dependencies...\")\n",
    "  # Use your GitHub username here\n",
    "  !pip install -q -r https://raw.githubusercontent.com/eriktaylor/ai-agent-moat/main/requirements.txt\n",
    "else:\n",
    "  print(\"Dependencies are already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Securely Set Up API Keys\n",
    "\n",
    "We need API keys for Google services. We'll use Colab's built-in **Secrets Manager** to handle these securely. This is the best practice and prevents you from ever exposing your keys in the notebook.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Click the **key icon (ðŸ”‘)** in the left sidebar of Colab.\n",
    "2.  Click **\"Add a new secret\"**.\n",
    "3.  Create a secret with the name `GOOGLE_API_KEY` and paste your Google AI Studio API key as the value.\n",
    "4.  Create another secret named `GOOGLE_CSE_ID` with your Custom Search Engine ID.\n",
    "\n",
    "You can get a Google API Key from [Google AI Studio](https://aistudio.google.com/app/apikey) and set up a Custom Search Engine [here](https://programmablesearchengine.google.com/controlpanel/all) to get a Search Engine ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Securely access the API keys\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID = userdata.get('GOOGLE_CSE_ID')\n",
    "\n",
    "# Set environment variables for LangChain\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Agent's Tools\n",
    "\n",
    "An agent's power comes from its tools. We will create three tools:\n",
    "1.  **Web Search Tool:** To find relevant articles and sources.\n",
    "2.  **Web Scraper Tool:** To extract the actual content from the URLs found by the search tool.\n",
    "3.  **Yahoo Finance Tool:** To fetch key financial metrics for public companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "from langchain.agents import tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "\n",
    "# Tool 1: Google Search Tool\n",
    "search = GoogleSearchAPIWrapper(google_cse_id=GOOGLE_CSE_ID, google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Upgraded scraper tool with PDF handling\n",
    "@tool\n",
    "def scrape_website(url: str) -> str:\n",
    "    \"\"\"Scrapes text from HTML websites and extracts text from PDF files.\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "\n",
    "        if 'application/pdf' in content_type or url.lower().endswith('.pdf'):\n",
    "            # It's a PDF, use PyMuPDF to extract text\n",
    "            with fitz.open(stream=io.BytesIO(response.content), filetype='pdf') as doc:\n",
    "                text = \"\".join(page.get_text() for page in doc)\n",
    "            return text[:8000] # Return a larger chunk for detailed PDFs\n",
    "        elif 'text/html' in content_type:\n",
    "            # It's HTML, use BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
    "            if len(text) < 200:\n",
    "                return f\"Error: HTML content from {url} is too short.\"\n",
    "            return text[:4000]\n",
    "        else:\n",
    "            return f\"Error: Unsupported content type '{content_type}' at {url}\"\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error: Could not access the URL. {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: An unexpected error occurred while processing {url}. {e}\"\n",
    "\n",
    "# Tool now fetches EPS\n",
    "@tool\n",
    "def get_stock_info(ticker: str) -> str:\n",
    "    \"\"\"Fetches key financial information for a given stock ticker using Yahoo Finance.\"\"\"\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        info = stock.info\n",
    "        market_cap = info.get('marketCap', 'N/A')\n",
    "        trailing_pe = info.get('trailingPE', 'N/A')\n",
    "        forward_pe = info.get('forwardPE', 'N/A')\n",
    "        eps = info.get('trailingEps', 'N/A')\n",
    "        long_business_summary = info.get('longBusinessSummary', 'N/A')\n",
    "        return f\"### KEY FINANCIAL DATA ###\\nMarket Cap: {market_cap}\\nTrailing P/E: {trailing_pe}\\nForward P/E: {forward_pe}\\nTrailing EPS: {eps}\\nBusiness Summary: {long_business_summary}\\n### END FINANCIAL DATA ###\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching stock info for {ticker}: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Research Agent\n",
    "\n",
    "The `ResearchAgent` class now encapsulates the entire research process, with distinct methods for different types of analysis. It also includes a caching mechanism to improve speed and avoid hitting API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import re\n",
    "\n",
    "class ResearchAgent:\n",
    "    def __init__(self, llm, embeddings_model):\n",
    "        self.llm = llm\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "        self.search_wrapper = GoogleSearchAPIWrapper(google_cse_id=GOOGLE_CSE_ID, google_api_key=GOOGLE_API_KEY)\n",
    "        self.cache = {}\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clears the agent's cache.\"\"\"\n",
    "        print(\"Cache cleared.\")\n",
    "        self.cache = {}\n",
    "\n",
    "    # <<< CHANGE: This method now separates financial data from unstructured text >>>\n",
    "    def _get_context(self, entity_name, ticker):\n",
    "        \"\"\"Gathers context and returns financial data and unstructured text separately.\"\"\"\n",
    "        context_cache_key = f\"context_{entity_name}_{ticker}\"\n",
    "        if context_cache_key in self.cache:\n",
    "            print(\"Returning cached context.\")\n",
    "            return self.cache[context_cache_key]\n",
    "\n",
    "        # Get financial data\n",
    "        financial_data = \"No financial data available.\"\n",
    "        if ticker:\n",
    "            print(f\"--- Getting Financial Data for {ticker} ---\")\n",
    "            financial_data_result = get_stock_info.run(ticker)\n",
    "            if not financial_data_result.startswith(\"Error\"):\n",
    "                financial_data = financial_data_result\n",
    "                print(\"Successfully collected financial data.\")\n",
    "\n",
    "        # Gather unstructured text\n",
    "        unstructured_text_list = []\n",
    "        print(\"--- Tier 1: Official News & Analysis ---\")\n",
    "        headline_query = f'\"{entity_name}\" recent news'\n",
    "        headline_results = self.search_wrapper.results(headline_query, num_results=3)\n",
    "        if headline_results:\n",
    "            unstructured_text_list.extend([f\"Headline: {r.get('title', '')}\\nSnippet: {r.get('snippet', '')}\" for r in headline_results])\n",
    "            print(f\"Collected {len(headline_results)} headlines and snippets.\")\n",
    "        \n",
    "        print(\"--- Tier 2: Critical News & Sentiment ---\")\n",
    "        critical_query = f'\\\"{entity_name}\\\" issues OR concerns OR investigation OR recall OR safety OR \"short interest\"'\n",
    "        critical_results = self.search_wrapper.results(critical_query, num_results=3)\n",
    "        if critical_results:\n",
    "            unstructured_text_list.extend([f\"Critical Headline: {r.get('title', '')}\\nSnippet: {r.get('snippet', '')}\" for r in critical_results])\n",
    "            print(f\"Collected {len(critical_results)} critical headlines and snippets.\")\n",
    "\n",
    "        print(\"--- Tier 3: Deep Dive ---\")\n",
    "        deep_dive_query = f'\\\"{entity_name}\\\" market analysis OR in-depth report filetype:pdf OR site:globenewswire.com OR site:prnewswire.com'\n",
    "        deep_dive_results = self.search_wrapper.results(deep_dive_query, num_results=2)\n",
    "        if deep_dive_results:\n",
    "            urls = [result['link'] for result in deep_dive_results if 'link' in result]\n",
    "            for url in urls:\n",
    "                print(f\"Scraping {url}...\")\n",
    "                content = scrape_website.run(url)\n",
    "                if content and not content.startswith(\"Error\"):\n",
    "                    unstructured_text_list.append(content)\n",
    "                    print(f\"Successfully scraped content from {url}\")\n",
    "                else:\n",
    "                    print(content)\n",
    "        \n",
    "        unstructured_corpus = \"\\n\\n---\\n\\n\".join(unstructured_text_list)\n",
    "        self.cache[context_cache_key] = (financial_data, unstructured_corpus)\n",
    "        return financial_data, unstructured_corpus\n",
    "\n",
    "    # <<< CHANGE: This method now takes financial data as a direct input >>>\n",
    "    def _create_rag_chain(self, system_prompt, unstructured_corpus):\n",
    "        \"\"\"Helper to create a RAG chain with a specific prompt.\"\"\"\n",
    "        docs = self.text_splitter.split_text(unstructured_corpus)\n",
    "        vector_store = FAISS.from_texts(texts=docs, embedding=self.embeddings_model)\n",
    "        retriever = vector_store.as_retriever()\n",
    "        # The prompt now has a dedicated placeholder for the guaranteed financial data\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)\n",
    "        return create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "    def generate_market_outlook(self, entity_name, ticker):\n",
    "        print(\"\\nGenerating Market Investor Outlook...\")\n",
    "        financial_data, unstructured_corpus = self._get_context(entity_name, ticker)\n",
    "        if not unstructured_corpus:\n",
    "            return \"Could not gather context for outlook generation.\"\n",
    "\n",
    "        # <<< CHANGE: Prompt now has a dedicated {financial_data} placeholder >>>\n",
    "        system_prompt = (\n",
    "            \"You are a 'Market Investor' analyst. Here is the key financial data for the company:\\n{financial_data}\\n\\n\" \n",
    "            \"Now, using the retrieved context below (which includes both positive and critical news), generate a report. \"\n",
    "            \"The report MUST be structured with the following sections:\\n\"\n",
    "            \"1. **Market Sentiment:** Synthesize the official news and the critical news to determine the overall market sentiment. Is it bullish, bearish, or mixed? Why?\\n\"\n",
    "            \"2. **Valuation Analysis:** Is the stock considered expensive or cheap? You MUST reference the 'Trailing P/E' and 'Trailing EPS' from the financial data. If P/E is not applicable because EPS is negative, state this clearly and explain what a negative EPS implies for valuation.\\n\"\n",
    "            \"3. **Relative Performance (Implied):** Based on the context, how does this company's performance and outlook seem to compare to its peers or the broader market?\"\n",
    "            \"Retrieved Context:\\n{context}\"\n",
    "            \"DO NOT give financial advice. This is an objective summary of the data provided.\"\n",
    "        )\n",
    "        rag_chain = self._create_rag_chain(system_prompt, unstructured_corpus)\n",
    "        # <<< CHANGE: We now pass the financial_data directly into the prompt >>>\n",
    "        response = rag_chain.invoke({\"input\": f\"Market outlook for {entity_name}\", \"financial_data\": financial_data})\n",
    "        return response['answer']\n",
    "\n",
    "    def generate_value_analysis(self, entity_name, ticker):\n",
    "        print(\"\\nGenerating Value Investor Analysis...\")\n",
    "        financial_data, unstructured_corpus = self._get_context(entity_name, ticker)\n",
    "        if not unstructured_corpus:\n",
    "            return \"Could not gather context for value analysis.\"\n",
    "\n",
    "        system_prompt = (\n",
    "            \"You are a 'Value Investor' analyst. Here is the key financial data for the company:\\n{financial_data}\\n\\n\" \n",
    "            \"Now, using the retrieved context below (which includes both positive and critical news), generate a detailed business brief. \"\n",
    "            \"The report MUST be structured with the following sections:\\n\"\n",
    "            \"1. **Valuation Summary:** Start by stating if the company appears 'Overvalued', 'Undervalued', or 'Fairly Valued'. Justify your conclusion briefly by referencing the P/E or EPS from the financial data.\\n\"\n",
    "            \"2. **SWOT Analysis:** A detailed, bulleted list of the company's Strengths, Weaknesses, Opportunities, and Threats. You MUST incorporate information from the 'Critical News' headlines in the Weaknesses and Threats sections.\\n\"\n",
    "            \"3. **Competitive Moat:** Based on the SWOT analysis, describe the company's long-term competitive advantages. Is its moat wide, narrow, or degrading? You MUST consider the threats and weaknesses when assessing the durability of the moat.\"\n",
    "            \"Retrieved Context:\\n{context}\"\n",
    "        )\n",
    "        rag_chain = self._create_rag_chain(system_prompt, unstructured_corpus)\n",
    "        response = rag_chain.invoke({\"input\": f\"Value analysis for {entity_name}\", \"financial_data\": financial_data})\n",
    "        return response['answer']\n",
    "\n",
    "# Initialize models and agent\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.2)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "research_agent = ResearchAgent(llm=llm, embeddings_model=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run the Three-Step Analysis\n",
    "\n",
    "Now we execute our new, structured workflow. First, we'll display the raw financial data. Then, we'll generate the investment outlook, and finally, the comprehensive business brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "company_name = input(\"Enter the company name (e.g., NVIDIA): \")\n",
    "stock_ticker = input(\"Enter the stock ticker (e.g., NVDA): \")\n",
    "\n",
    "clear_output(wait=True) # Clears the input prompts for a cleaner display\n",
    "\n",
    "# To get fresh data and not use the cache, you can uncomment the next line:\n",
    "# research_agent.clear_cache()\n",
    "\n",
    "# --- Step 1: Display Raw Financial Data ---\n",
    "print(f\"--- 1. KEY FINANCIAL DATA for {stock_ticker.upper()} ---\")\n",
    "financial_data_raw = get_stock_info.run(stock_ticker) if stock_ticker else \"No ticker provided.\"\n",
    "display(HTML(f\"<div style='border: 1px solid #444; border-radius: 8px; padding: 20px; white-space: pre-wrap; font-family: monospace; line-height: 1.6; background-color: #2c2c2e; color: #f0f0f0;'>{financial_data_raw}</div>\"))\n",
    "\n",
    "# --- Step 2: Generate Market Investor Outlook ---\n",
    "print(f\"\\n--- 2. AI-GENERATED MARKET INVESTOR OUTLOOK for {company_name} ---\")\n",
    "market_outlook = research_agent.generate_market_outlook(company_name, stock_ticker)\n",
    "display(HTML(f\"<div style='border: 1px solid #444; border-radius: 8px; padding: 20px; max-height: 500px; overflow-y: auto; white-space: pre-wrap; font-family: \\\"SF Pro Text\\\", \\\"Inter\\\", sans-serif; line-height: 1.6; background-color: #2c2c2e; color: #f0f0f0;'>{market_outlook}</div>\"))\n",
    "\n",
    "# --- Step 3: Generate Value Investor Analysis ---\n",
    "print(f\"\\n--- 3. AI-GENERATED VALUE INVESTOR ANALYSIS for {company_name} ---\")\n",
    "value_analysis = research_agent.generate_value_analysis(company_name, stock_ticker)\n",
    "display(HTML(f\"<div style='border: 1px solid #444; border-radius: 8px; padding: 20px; max-height: 500px; overflow-y: auto; white-space: pre-wrap; font-family: \\\"SF Pro Text\\\", \\\"Inter\\\", sans-serif; line-height: 1.6; background-color: #2c2c2e; color: #f0f0f0;'>{value_analysis}</div>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
