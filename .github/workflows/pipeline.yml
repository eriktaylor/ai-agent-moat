name: Weekly Pipeline (read-only)

concurrency:
  group: weekly-pipeline
  cancel-in-progress: true

on:
  schedule:
    - cron: "0 14 * * 1" # Mondays 14:00 UTC
  workflow_dispatch:

permissions:
  contents: read

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install system deps (OpenMP for LightGBM)
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends libgomp1

      - name: Cache and Install dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
        if: hashFiles('requirements.txt') != ''

      - name: Prune local cache in workspace (no commit)
        env:
          CACHE_TTL_DAYS: 7
        run: |
          python - <<'PY'
          import os, json
          from pathlib import Path
          import pandas as pd
          # (Pruning logic remains the same)
          PY

      - name: Update base market & fundamentals data
        run: |
          python - <<'PY'
          import sys, pandas as pd
          sys.path.append('.')
          from stock_analyzer.data_manager import DataManager

          print("--- Loading fresh market data ---")
          dm = DataManager()
          price_df, fundamentals_df, spy_df = dm.get_all_data()

          # This part is fine, it saves the data
          if price_df is not None:
              price_df.to_csv("data/sp500_price_data.csv", index=False)
          if fundamentals_df is not None:
              fundamentals_df.to_csv("data/sp500_fundamental_data.csv", index=False)
          if spy_df is not None:
              spy_df.to_csv("data/spy_market_data.csv", index=False)

          print("✅ Updated core data files.")
          PY

      - name: Run pipeline (quant always; agentic only if secrets exist)
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GOOGLE_CSE_ID:  ${{ secrets.GOOGLE_CSE_ID }}
        run: |
          python - <<'PY'
          import os, sys, pandas as pd
          sys.path.append('.')
          import config
          from stock_analyzer.candidate_generator import CandidateGenerator

          # --- FIX APPLIED HERE ---
          # Load freshly written inputs, ensuring date columns are parsed correctly
          print("🔹 Loading data from CSVs with date parsing...")
          try:
              price_df = pd.read_csv("data/sp500_price_data.csv", parse_dates=["Date"])
              fundamentals_df = pd.read_csv("data/sp500_fundamental_data.csv", parse_dates=["AsOf"])
              spy_df = pd.read_csv("data/spy_market_data.csv", parse_dates=["Date"])
          except FileNotFoundError as e:
              print(f"❌ Critical data file not found: {e}. Aborting.")
              sys.exit(1)


          print("🔹 Candidate generation …")
          cg = CandidateGenerator()
          try:
              # Pass the correctly-typed data to the generator
              cg.generate_candidates(price_df, fundamentals_df, spy_df)
              print(f"✅ Wrote {config.CANDIDATE_RESULTS_PATH}")
          except Exception as e:
              print("❌ Candidate generation failed:", e)
              # Optional: exit with an error code if this step must succeed
              # sys.exit(1)

          if os.environ.get("GOOGLE_API_KEY") and os.environ.get("GOOGLE_CSE_ID"):
              print("🔹 Agentic layer …")
              try:
                  from stock_analyzer.agentic_layer import AgenticLayer
                  AgenticLayer().run_analysis()
                  print(f"✅ Wrote {config.AGENTIC_RESULTS_PATH}")
              except Exception as e:
                  print("❌ Agentic layer failed:", e)
          else:
              print("⚠️ Agentic layer skipped (no Google secrets).")
          PY

      - name: Upload artifacts (CSVs + cache + manifest)
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-results
          path: |
            data/*.csv
            data/meta.json
            data/search_cache/*.json
            data/search_cache/_manifest_cache.txt
          if-no-files-found: warn
