name: Weekly Pipeline (read-only)

concurrency:
  group: weekly-pipeline
  cancel-in-progress: true

on:
  schedule:
    - cron: "0 14 * * 1" # Mondays 14:00 UTC
  workflow_dispatch:

permissions:
  contents: read

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install system deps (OpenMP for LightGBM)
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends libgomp1

      - name: Cache and Install dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
        if: hashFiles('requirements.txt') != ''

      - name: Prune local cache in workspace (no commit)
        env:
          CACHE_TTL_DAYS: 7
        run: |
          python - <<'PY'
          import os, json
          from pathlib import Path
          import pandas as pd
          # (Pruning logic remains the same)
          PY

      - name: Update base market & fundamentals data
        run: |
          python - <<'PY'
          import sys, pandas as pd
          sys.path.append('.')
          from stock_analyzer.data_manager import DataManager

          print("--- Loading fresh market data ---")
          dm = DataManager()
          price_df, fundamentals_df, spy_df = dm.get_all_data()

          # This part is fine, it saves the data
          if price_df is not None:
              price_df.to_csv("data/sp500_price_data.csv", index=False)
          if fundamentals_df is not None:
              fundamentals_df.to_csv("data/sp500_fundamental_data.csv", index=False)
          if spy_df is not None:
              spy_df.to_csv("data/spy_market_data.csv", index=False)

          print("‚úÖ Updated core data files.")
          PY

      - name: Run pipeline (quant always; agentic only if secrets exist)
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GOOGLE_CSE_ID:  ${{ secrets.GOOGLE_CSE_ID }}
        run: |
          python - <<'PY'
          import os, sys, pandas as pd
          from datetime import datetime
          import re
          sys.path.append('.')
          import config
          from stock_analyzer.candidate_generator import CandidateGenerator

          # --- FIX APPLIED HERE ---
          # Load freshly written inputs, ensuring date columns are parsed correctly
          print("üîπ Loading data from CSVs with date parsing...")
          try:
              price_df = pd.read_csv("data/sp500_price_data.csv", parse_dates=["Date"])
              fundamentals_df = pd.read_csv("data/sp500_fundamental_data.csv", parse_dates=["AsOf"])
              spy_df = pd.read_csv("data/spy_market_data.csv", parse_dates=["Date"])
          except FileNotFoundError as e:
              print(f"‚ùå Critical data file not found: {e}. Aborting.")
              sys.exit(1)


          print("üîπ Candidate generation ‚Ä¶")
          cg = CandidateGenerator()
          try:
              # Pass the correctly-typed data to the generator
              cg.generate_candidates(price_df, fundamentals_df, spy_df)
              print(f"‚úÖ Wrote {config.CANDIDATE_RESULTS_PATH}")
          except Exception as e:
              print("‚ùå Candidate generation failed:", e)
              # Optional: exit with an error code if this step must succeed
              # sys.exit(1)

          if os.environ.get("GOOGLE_API_KEY") and os.environ.get("GOOGLE_CSE_ID"):
              print("üîπ Agentic layer ‚Ä¶")
              try:
                  from stock_analyzer.agentic_layer import AgenticLayer
                  AgenticLayer().run_analysis()
                  print(f"‚úÖ Wrote {config.AGENTIC_RESULTS_PATH}")
              except Exception as e:
                  print("‚ùå Agentic layer failed:", e)
          else:
              print("‚ö†Ô∏è Agentic layer skipped (no Google secrets).")

          # --- Run the Portfolio Manager ---
          if os.environ.get("GOOGLE_API_KEY") and os.environ.get("GOOGLE_CSE_ID"):
              print("üîπ Portfolio Manager ‚Ä¶")
              try:
                  from stock_analyzer.portfolio_manager import PortfolioManager
                  quant_df = pd.read_csv(config.CANDIDATE_RESULTS_PATH)
                  agentic_df = pd.read_csv(config.AGENTIC_RESULTS_PATH)
    
                  pm = PortfolioManager()
                  thesis, new_portfolio_df = pm.generate_portfolio(quant_df, agentic_df)
                  
                  if not new_portfolio_df.empty:
                      new_portfolio_df.to_csv(config.PORTFOLIO_PATH, index=False)
                      print(f"‚úÖ Wrote portfolio to {config.PORTFOLIO_PATH}")
    
                  print("üîπ Updating README.md with new thesis...")
                  readme_path = 'README.md'
                  try:
                      with open(readme_path, 'r', encoding='utf-8') as f:
                          readme_content = f.read()

                      # --- 1. Update Thesis Content ---
                      today_str = datetime.now().strftime('%Y-%m-%d')
                      new_thesis_content = (
                          f"*This section is automatically updated by the AI Portfolio Manager. Last updated: {today_str}.*\n\n"
                          f"{thesis}"
                      )
                      readme_content = re.sub(
                          r"(?s)<!-- START: PORTFOLIO_THESIS -->.*<!-- END: PORTFOLIO_THESIS -->",
                          f"<!-- START: PORTFOLIO_THESIS -->\n{new_thesis_content}\n<!-- END: PORTFOLIO_THESIS -->",
                          readme_content
                      )

                      # --- 2. NEW: Update Portfolio Summary Content ---
                      portfolio_summary = ""
                      if not new_portfolio_df.empty:
                          total_weight = new_portfolio_df['Weight_%'].sum()
                          for _, row in new_portfolio_df.iterrows():
                              portfolio_summary += f"{row['Ticker']}: {row['Weight_%']:.1f}%\n"
                          cash_weight = 100 - total_weight
                          portfolio_summary += f"U.S. Treasury Bills (Cash Equivalent): {cash_weight:.1f}%"
                      else:
                          portfolio_summary = "100% in U.S. Treasury Bills (Cash Equivalent)"
                      
                      readme_content = re.sub(
                          r"(?s)<!-- START: PORTFOLIO_SUMMARY -->.*<!-- END: PORTFOLIO_SUMMARY -->",
                          f"<!-- START: PORTFOLIO_SUMMARY -->\n{portfolio_summary}\n<!-- END: PORTFOLIO_SUMMARY -->",
                          readme_content
                      )
    
                      with open(readme_path, 'w', encoding='utf-8') as f:
                          f.write(readme_content)
                      
                      print("‚úÖ README.md updated successfully with thesis and portfolio.")          
                  except Exception as e:
                      print(f"‚ùå Failed to update README.md: {e}")
    
              except Exception as e:
                  print(f"‚ùå Portfolio Manager failed: {e}")
          else:
              print("‚ö†Ô∏è Agentic layer skipped (no Google secrets).")
          
          PY

      - name: Upload artifacts (CSVs + cache + manifest)
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-results
          path: |
            data/*.csv
            data/meta.json
            data/search_cache/*.json
            data/search_cache/_manifest_cache.txt
            README.md
          if-no-files-found: warn
