# .github/workflows/main.yml

name: Weekly Investment Pipeline

on:
  schedule:
    # Every Monday 14:00 UTC (adjust if you want a different time)
    - cron: '0 14 * * 1'
  workflow_dispatch:

permissions:
  contents: write   # needed to push cache/CSV updates

jobs:
  prune-cache:
    name: Prune search cache (TTL)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Prune cache files older than TTL
        env:
          CACHE_TTL_DAYS: 7  # change if you want a longer/shorter TTL
        run: |
          python - <<'PY'
          import os, json
          from pathlib import Path
          import pandas as pd

          TTL_DAYS = int(os.getenv("CACHE_TTL_DAYS", "7"))
          CACHE_DIR = Path("data/search_cache")
          CACHE_DIR.mkdir(parents=True, exist_ok=True)

          def load(p):
            try:
              with open(p, "r", encoding="utf-8") as f:
                payload = json.load(f)
              if isinstance(payload, dict) and "results" in payload:
                return payload
              return {"_meta": {}, "results": payload}
            except Exception:
              return None

          pruned = 0
          for p in CACHE_DIR.glob("*.json"):
            payload = load(p)
            if payload is None:
              # unreadable; prune
              p.unlink(missing_ok=True)
              pruned += 1
              continue
            meta = payload.get("_meta", {})
            try:
              created = pd.to_datetime(meta.get("created_utc"))
              if pd.isna(created):
                raise ValueError("no created_utc")
              age = pd.Timestamp.utcnow() - created
              if age > pd.Timedelta(days=TTL_DAYS):
                p.unlink()
                pruned += 1
            except Exception:
              # missing or bad _meta -> prune
              p.unlink(missing_ok=True)
              pruned += 1

          print(f"Pruned {pruned} cache files older than {TTL_DAYS} days.")
          PY

      - name: Commit & Push cache pruning (if any)
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "🧹 Prune search cache (TTL)"
          file_pattern: "data/search_cache/*.json"

  run-pipeline:
    name: Run candidate gen + agentic layer
    runs-on: ubuntu-latest
    needs: prune-cache

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies (if requirements.txt exists)
        if: hashFiles('requirements.txt') != ''
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run pipeline (candidate gen always; agentic layer only if secrets exist)
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}
        run: |
          python - <<'PY'
          import os
          import pandas as pd

          # Ensure project imports work
          import sys
          sys.path.append('.')

          import config

          # Import your modules
          from stock_analyzer.candidate_generator import CandidateGenerator
          # Agentic layer relies on Google Search + Gemini; run only if both secrets present
          google_ok = bool(os.environ.get('GOOGLE_API_KEY')) and bool(os.environ.get('GOOGLE_CSE_ID'))
          if google_ok:
              from stock_analyzer.agentic_layer import AgenticLayer

          # Load inputs (paths expected from your repo structure)
          price_path = "data/sp500_price_data.csv"
          funda_path = "data/sp500_fundamental_data.csv"
          spy_path   = "data/spy_market_data.csv"

          print("🔹 Loading input data…")
          price_df = pd.read_csv(price_path)
          fundamentals_df = pd.read_csv(funda_path)
          spy_df = pd.read_csv(spy_path)

          print("🔹 Running candidate generation…")
          cg = CandidateGenerator()
          try:
              top_cands, feat_imp = cg.generate_candidates(price_df, fundamentals_df, spy_df)
              print(f"✅ Candidates saved to {config.CANDIDATE_RESULTS_PATH}")
          except Exception as e:
              # Don't fail the pipeline if quant step breaks—just log
              print(f"❌ Candidate generation failed: {e}")

          if google_ok:
              print("🔹 Running agentic layer (LLM + Search)…")
              try:
                  agent = AgenticLayer()
                  df = agent.run_analysis()
                  print(f"✅ Agentic recommendations saved to {config.AGENTIC_RESULTS_PATH}")
              except Exception as e:
                  print(f"❌ Agentic layer failed: {e}")
          else:
              print("⚠️ Skipping agentic layer: GOOGLE_API_KEY / GOOGLE_CSE_ID not configured as repo secrets.")

          PY

      - name: Commit & Push outputs (if any changed)
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "📈 Weekly pipeline: candidates, agentic results, cache"
          file_pattern: |
            data/*.csv
            data/search_cache/*.json
